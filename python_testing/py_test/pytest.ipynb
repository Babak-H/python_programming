{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3cd3d141001d7239f5f46ab1074ae7f74ecb5ff09dce6af296526824bec9bc55"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_convert_to_int.py\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import convert_to_int\n",
    "\n",
    "def test_on_string_with_one_comma():\n",
    "    test_argument = \"2,081\"\n",
    "    expected = 2081\n",
    "    actual = convert_to_int(test_argument)\n",
    "    # Format the string with the actual return value\n",
    "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
    "    # Write the assert statement which prints message on failure\n",
    "    assert actual == expected, message"
   ]
  },
  {
   "source": [
    "If the return value is a float or an object containing float, you should use the pytest.approx() function in comparisons.<br><br>\n",
    "assert 0.1 + 0.1 + 0.1 == pytest.approx(0.3)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_get_data_as_numpy_array.py\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "from as_numpy import get_data_as_numpy_array\n",
    "\n",
    "def test_on_clean_file():\n",
    "  expected = np.array([[2081.0, 314942.0],\n",
    "                       [1059.0, 186606.0],\n",
    "  \t\t\t\t\t   [1148.0, 206186.0]])\n",
    "                         \n",
    "  actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "  message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "  # Complete the assert statement\n",
    "  assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_split_into_training_and_testing_sets.py\n",
    "\n",
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                 [1210.0, 214114.0], [1697.0, 277794.0]])\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = 2\n",
    "    actual = split_into_training_and_testing_sets(example_argument)\n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == expected_training_array_num_rows, f\"The actual number of rows in the training array is not {expected_training_array_num_rows}\"\n",
    "    # Write the assert statement checking testing array's number of rows\n",
    "    assert actual[1].shape[0] == expected_testing_array_num_rows, f\"The actual number of rows in the testing array is not {expected_testing_array_num_rows}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import row_to_list\n",
    "\n",
    "def test_for_clean_row():\n",
    "  assert row_to_list(\"2,081\\t314,942\\n\") == [\"2,081\", \"314,942\"]\n",
    "\n",
    "def test_for_missing_area():\n",
    "  assert row_to_list(\"\\t293,410\\n\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    test_argument = np.array([[1.0, 0.0], \n",
    "                              [cos(theta), sin(theta)],[0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],[-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],[0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]])\n",
    "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
    "    assert actual == pytest.approx(0.0)"
   ]
  },
  {
   "source": [
    " with context_manager:\n",
    "    # <--- Runs code on entering context\n",
    "    print(\"This is part of the context\")    # any code inside is the context\n",
    "    # <--- Runs code on exiting context\n",
    "\n",
    "\n",
    " with pytest.raises(ValueError):\n",
    "    # <--- Does nothing on entering the context\n",
    "    print(\"This is part of the context\")\n",
    "    # <--- If context raised ValueError, the code IS CORRECT\n",
    "    # <--- If the context did not raise ValueError, raise an exception.\n",
    "\n",
    "\n",
    " with pytest.raises(ValueError):\n",
    "    raise ValueError    # context exits with ValueError\n",
    "    # <--- pytest.raises(ValueError) silences it\n",
    "\n",
    "\n",
    " with pytest.raises(ValueError):\n",
    "    pass    # context exits without raising a ValueError\n",
    "    # <--- pytest.raises(ValueError) raises Failed\n",
    "\n",
    "\n",
    "# Store the raised ValueError in the variable exc_info\n",
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "\n",
    "\n",
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "# Check if the raised ValueError contains the correct message\n",
    "assert exc_info.match(\"Silence me!\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def test_valueerror_on_one_dimensional_argument():\n",
    "    example_argument = np.array([2081, 314942, 1059, 186606, 1148, 206186])\n",
    "    \n",
    "    with pytest.raises(ValueError):\n",
    "        split_into_training_and_testing_sets(example_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def test_valueerror_on_one_dimensional_argument():\n",
    "    example_argument = np.array([2081, 314942, 1059, 186606, 1148, 206186])\n",
    "\n",
    "    with pytest.raises(ValueError) as exception_info:    # store the exception\n",
    "        split_into_training_and_testing_sets(example_argument)\n",
    "\n",
    "    # Check if ValueError contains correct message\n",
    "    assert exception_info.match(\"Argument data array must be two dimensional. \", \"Got 1 dimensional array instead!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from train import split_into_training_and_testing_sets\n",
    "\n",
    "def test_on_one_row():\n",
    "    test_argument = np.array([[1382.0, 390167.0]])\n",
    "\n",
    "    # Store information about raised ValueError in exc_info\n",
    "    with pytest.raises(ValueError) as exc_info:\n",
    "      split_into_training_and_testing_sets(test_argument)\n",
    "      \n",
    "    expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "    # Check if the raised ValueError contains the correct message\n",
    "    assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n",
    "    actual = row_to_list(\"123\\n\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "\n",
    "    \n",
    "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
    "    actual = row_to_list(\"\\t4,567\\n\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_normal_argument_1():\n",
    "    actual = row_to_list(\"123\\t4,567\\n\")\n",
    "    expected = [\"123\", \"4,567\"]\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TDD\n",
    "first write several test functions to test the proposed method in different ways\n",
    "'''\n",
    "\n",
    "def test_with_no_comma():\n",
    "    actual = convert_to_int(\"756\")\n",
    "    assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n",
    "    \n",
    "\n",
    "def test_with_one_comma():\n",
    "    actual = convert_to_int(\"2,081\")\n",
    "    assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
    "\n",
    "\n",
    "def test_on_string_with_incorrectly_placed_comma():\n",
    "    actual = convert_to_int(\"12,72,891\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "\n",
    "def test_on_float_valued_string():\n",
    "    actual = convert_to_int(\"23,816.92\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "then write the function that would pass all those tests\n",
    "'''\n",
    "\n",
    "def convert_to_int(integer_string_with_commas):\n",
    "    comma_separated_parts = integer_string_with_commas.split(\",\")\n",
    "    for i in range(len(comma_separated_parts)):\n",
    "        if len(comma_separated_parts[i]) > 3:\n",
    "            return None\n",
    "        if i != 0 and len(comma_separated_parts[i]) != 3:\n",
    "            return None\n",
    "    integer_string_without_commas = \"\".join(comma_separated_parts)\n",
    "    try:\n",
    "        return int(integer_string_without_commas)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test class is a container for a single unit's testing\n",
    "\n",
    "from data.preprocessing_helpers import row_to_list, convert_to_int\n",
    "\n",
    "class TestRowToList(object):  # Always put the argument object\n",
    "                        \n",
    "    def test_on_no_tab_no_missing_value(self):    # Always put the argument self\n",
    "        pass\n",
    "\n",
    "    def test_on_two_tabs_no_missing_value(self):  # Always put the argument self\n",
    "        pass\n",
    "\n",
    "# you can keep two test classes inside one file (similar to a python module)\n",
    "class TestConvertToInt(object):\n",
    "\n",
    "    def test_with_no_comma(self):\n",
    "        pass\n",
    "\n",
    "    def test_with_one_comma(self):\n",
    "        pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "\n",
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "\n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "            \n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "source": [
    "'''\n",
    "How to run all tests in the test folder\n",
    "\n",
    "a =>  cd into the test folder\n",
    "b => run the command \"pytest\"\n",
    "pytest will automatically find all the needed tests and run them\n",
    "'''\n",
    "\n",
    "'''\n",
    "for a specific test class or test function\n",
    "\n",
    "Node ID of a test class =>  <path to test module>::<test class name>\n",
    "Node ID of a test function => <path to test module>::<test class name>::<unit test name>\n",
    "'''\n",
    ">> pytest data/test_preprocessing_helpers.py::TestRowToList\n",
    ">> pytest data/test_preprocessing_helpers.py::TestRowToList::test_on_one_tab_with_missing_value"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-d7abc0e1f0da>, line 15)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-d7abc0e1f0da>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    pytest data/test_preprocessing_helpers.py::TestRowToList\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_into_training_and_testing_sets(data_array):\n",
    "    dim = data_array.ndim\n",
    "    if dim != 2:\n",
    "        raise ValueError(\"Argument data_array must be two dimensional. Got {0} dimensional array instead!\".format(dim))\n",
    "\n",
    "    num_rows = data_array.shape[0]\n",
    "    if num_rows < 2:\n",
    "        raise ValueError(\"Argument data_array must have at least 2 rows, it actually has just {0}\".format(num_rows))\n",
    "\n",
    "    num_training = int(0.75 * data_array.shape[0])\n",
    "    permuted_indices = np.random.permutation(data_array.shape[0])\n",
    "\n",
    "    return data_array[permuted_indices[:num_training], :], data_array[permuted_indices[num_training:], :]\n",
    "\n",
    "\n",
    ">> pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets\n",
    "\n",
    "# how to run a specific test that was failing before\n",
    ">> pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows\n",
    "\n",
    "# the -k expression finds the closest named class/method available in current folder\n",
    ">> pytest -k \"SplitInto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "class TestTrainModel(object):\n",
    "    # this annotation will inform that this test is supposed to fail\n",
    "    @pytest.mark.xfail\n",
    "    def test_on_linear_data(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\n",
    "class TestModelTest(object):\n",
    "    def test_on_linear_data(self):\n",
    "        pass\n",
    "    \n",
    "    def test_on_one_dimensional_array(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    # Add a reason for skipping the test\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower\")\n",
    "    def test_on_clean_file(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the failure reseaon in test failed results\n",
    ">>  pytest -r\n",
    "\n",
    "# would only show the reason for skipped tests in the test result report\n",
    ">> pytest -rs"
   ]
  },
  {
   "source": [
    "Some functions in python need to process some file or need input that requires some pre-process<br><br>\n",
    "to test these functions we need to : create the needed file (SetUp) -> do the testing -> delete the tested file (TearDown)<br><br>\n",
    "the extra method for this testing is annotated with \"@pytest.fixture\" \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def my_fixture():\n",
    "    # Do setup here\n",
    "    yield data    # Use yield instead of return\n",
    "    # Do teardown here\n",
    "\n",
    "\n",
    "def test_something(my_fixture):\n",
    "    ...\n",
    "    data = my_fixture\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real life example\n",
    "\n",
    "@pytest.fixture\n",
    "def raw_and_clean_data_file():\n",
    "    #setup part\n",
    "    raw_data_file_path = \"raw.txt\"\n",
    "    clean_data_file_path = \"clean.txt\"\n",
    "    with open(raw_data_file_path, \"w\") as f:\n",
    "        f.write(\"1,801\\t201,411\\n\"\n",
    "                \"1,767565,112\\n\"\n",
    "                \"2,002\\t333,209\\n\"\n",
    "                \"1990\\t782,911\\n\"\n",
    "                \"1,285\\t389129\\n\")\n",
    "    # part to give the needed file to the test function\n",
    "    yield raw_data_file_path, clean_data_file_path\n",
    "    # teardown part\n",
    "    os.remove(raw_data_file_path)\n",
    "    os.remove(clean_data_file_path)\n",
    "\n",
    "\n",
    "def test_on_raw_data(raw_and_clean_data_file):\n",
    "    # call the fixture method\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # run the tested function\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # do the test assertion\n",
    "    with open(clean_data_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\t201411\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\t333209\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def empty_file():\n",
    "    # Assign the file path \"empty.txt\" to the variable\n",
    "    file_path = \"empty.txt\"\n",
    "    open(file_path, \"w\").close()\n",
    "    # Yield the variable file_path\n",
    "    yield file_path\n",
    "    # Remove the file in the teardown\n",
    "    os.remove(file_path)\n",
    "    \n",
    "    \n",
    "def test_on_empty_file(self, empty_file):\n",
    "    expected = np.empty((0, 2))\n",
    "    actual = get_data_as_numpy_array(empty_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mock Tests\n",
    "\n",
    "when a function depends on other function to run properly, there might be a case when other method might have bugs\n",
    "this will fail our test, while its NOT the fault of the function we want to test.\n",
    "in such cases we make another function in testing to simulate the needed methods, to make sure they send us correct results\n",
    "this is called Mocking\n",
    "'''\n",
    "# install the pytest-mock package\n",
    ">> pip install pytest-mock\n",
    "# import unittest.mock package from python standard library\n",
    "\n",
    "\n",
    "# this is the mocked method\n",
    "def convert_to_int_bug_free(comma_separated_integer_string):\n",
    "    # Assign to the dictionary holding the correct return values\n",
    "    return_values = {\"1,801\": 1801,\n",
    "                     \"201,411\": 201411,\n",
    "                     \"2,002\": 2002,\n",
    "                     \"333,209\": 333209,\n",
    "                     \"1990\": None,\n",
    "                     \"782,911\": 782911,\n",
    "                     \"1,285\": 1285,\n",
    "                     \"389129\": None}\n",
    "    # Return the correct result using the dictionary return_values\n",
    "    return return_values[comma_separated_integer_string]\n",
    "\n",
    "\n",
    "# Add the correct argument to use the mocking fixture (mocker) in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\", side_effect=convert_to_int_bug_free)\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"),\n",
    "                                                  call(\"1990\"),  call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "    # open the file and read it to make sure the results are correct\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  }
 ]
}